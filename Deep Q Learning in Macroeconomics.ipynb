{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Most dynamic equilbrium models in economics do not have closed form solutions.  Instead, numerical methods are used to approximate their behavior. Here I show how Deep-Q Learning with a Natural Advantage Function can be used to approximate the solution to dynamic equilibrium models.\n",
    "\n",
    "First, I review the neoclassical growth model.  Then I show how to solve the model using the Value Iteration Approach.  Finally, I show how to solve the model using Deep-Q Learning with the Natural Advantage Function.  In the future, I'll compare the accuracy of the Deep-Q Learning to Peturbation Methods and the Value Iteration Approach on more complicated models.\n",
    "\n",
    "Useful background material is \"Comparing Solution Methods for Dynamic Equilibrium Economies\" (https://www.econstor.eu/bitstream/10419/100716/1/wp2003-27.pdf) which describes the relative accuracy existing numerical solution methods.  \"Continuous Deep Q-Learning with Model-based Acceleration\" (https://arxiv.org/pdf/1603.00748.pdf) introduces the Natural Advantage function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Neoclassical Model\n",
    "\n",
    "The neoclassical growth model was introduced in the 40s to describe long-term growth of economies. It involves a planner for the economy.  The planner faces a bread vs butter problem. She has some capital today.  She needs to decide how much capital society should consume today vs how much capital she should invest so people can consume tomorrow.\n",
    "\n",
    "The two building blocks are the utility function: how happy society is from consuming $c_t$.  And the production function, how much can society produce given capital $k_t$\n",
    "\n",
    "Given capital $k_t$ in period $t$, society can produce $z_tk_t^{\\alpha}$ capital the next period.  Where $\\alpha < 1$, there are diminishing returns. And there is a technology $z_t>0$.  Capital deprecates at rate $\\delta$. Therefore the production function looks like\n",
    "\n",
    "$$z_tk_t^{\\alpha} + (1- \\delta)k_t$$\n",
    "\n",
    "The utility function is the $\\log$ of consumption, eg there are diminishing returns to consuming.  In more complicated models there would also be leisure and a return on leisure.  Putting these building blocks together the planners problem looks like\n",
    "\n",
    "$$ \\max_{c_t, k_{t+1}} \\mathbb{E} \\sum_{t=0}^{\\inf} (1-\\beta)\\beta^t \\log{c_t}$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$c_t + k_{t+1} = z_tk_t^{\\alpha} + (1- \\delta)k_t$$ \n",
    "\n",
    "The planners problem is to choose $c_t$ and $k_{t+1}$ to maximize utility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Value Iteration Approach\n",
    "\n",
    "Aruoba and Fern√°ndez-Villaverde solve the problem using the Value Iteration Approach here https://github.com/jesusfv/Comparison-Programming-Languages-Economics/blob/master/RBC_Python.py\n",
    "\n",
    "The difficulty in useing the \"Value Iteration Method\" is that the state space and decision spaces are continious.  But value itearation is discrete.  The simplest approach reproduced below is to just discretize the space.  That approach is the most accurate.  But also doesn't scale computationally, the complexity increases exponentially with the complexity of the state space. \n",
    "\n",
    "The code is reproduced below.  First, we define the dynamics of the economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "aalpha = 1.0/3.0     # Elasticity of output w.r.t. capital\n",
    "bbeta  = 0.95        # Discount factor\n",
    "\n",
    "# Productivity values\n",
    "vProductivity = np.array([0.9792, 0.9896, 1.0000, 1.0106, 1.0212],float)\n",
    "\n",
    "# Transition matrix\n",
    "mTransition = np.array([[0.9727, 0.0273, 0.0000, 0.0000, 0.0000],\n",
    "                     [0.0041, 0.9806, 0.0153, 0.0000, 0.0000],\n",
    "                     [0.0000, 0.0082, 0.9837, 0.0082, 0.0000],\n",
    "                     [0.0000, 0.0000, 0.0153, 0.9806, 0.0041],\n",
    "                     [0.0000, 0.0000, 0.0000, 0.0273, 0.9727]],float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then we reproduce the grids used to solve solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output =  0.562731433871  Steady State Capital =  0.178198287393   Steady State Consumption =  0.384533146479\n",
      "len vGridCapital 17820 dim mPolicyFunction (17820, 5)\n"
     ]
    }
   ],
   "source": [
    "capitalSteadyState     = (aalpha*bbeta)**(1/(1-aalpha))\n",
    "outputSteadyState      = capitalSteadyState**aalpha\n",
    "consumptionSteadyState = outputSteadyState-capitalSteadyState\n",
    "\n",
    "# We generate the grid of capital\n",
    "vGridCapital           = np.arange(0.5*capitalSteadyState,1.5*capitalSteadyState,0.00001)\n",
    "\n",
    "nGridCapital           = len(vGridCapital)\n",
    "nGridProductivity      = len(vProductivity)\n",
    "\n",
    "## 3. Required matrices and vectors\n",
    "\n",
    "mOutput           = np.zeros((nGridCapital,nGridProductivity),dtype=float)\n",
    "mValueFunction    = np.zeros((nGridCapital,nGridProductivity),dtype=float)\n",
    "mValueFunctionNew = np.zeros((nGridCapital,nGridProductivity),dtype=float)\n",
    "mPolicyFunction   = np.zeros((nGridCapital,nGridProductivity),dtype=float)\n",
    "expectedValueFunction = np.zeros((nGridCapital,nGridProductivity),dtype=float)\n",
    "\n",
    " # 4. We pre-build output for each point in the grid\n",
    "\n",
    "for nProductivity in range(nGridProductivity):\n",
    "    mOutput[:,nProductivity] = vProductivity[nProductivity]*(vGridCapital**aalpha)\n",
    "\n",
    "print \"Output = \", outputSteadyState, \" Steady State Capital = \", capitalSteadyState, \"  Steady State Consumption = \", consumptionSteadyState\n",
    "print \"len vGridCapital\", nGridCapital, \"dim mPolicyFunction\", mPolicyFunction.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration =  1 , Sup Diff =  9.23064875646e-08\n",
      " Iteration =  <built-in function iter> , Sup Duff =  9.23064875646e-08\n",
      " \n",
      " My Check =  0.146549143696\n",
      " \n",
      "Elapse time = is  0.311843156815\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "maxDifference = 10.0\n",
    "tolerance = 0.0000001\n",
    "iteration = 0\n",
    "\n",
    "log = math.log\n",
    "zeros = np.zeros\n",
    "dot = np.dot\n",
    "\n",
    "while(maxDifference > tolerance):\n",
    "    expectedValueFunction = dot(mValueFunction,mTransition.T)\n",
    "\n",
    "    for nProductivity in xrange(nGridProductivity):\n",
    "\n",
    "        # We start from previous choice (monotonicity of policy function)\n",
    "        gridCapitalNextPeriod = 0\n",
    "\n",
    "        for nCapital in xrange(nGridCapital):\n",
    "\n",
    "            valueHighSoFar = -100000.0\n",
    "            capitalChoice  = vGridCapital[0]\n",
    "\n",
    "            for nCapitalNextPeriod in xrange(gridCapitalNextPeriod,nGridCapital):\n",
    "                consumption = mOutput[nCapital,nProductivity] - vGridCapital[nCapitalNextPeriod]\n",
    "                valueProvisional = (1-bbeta)*log(consumption)+bbeta*expectedValueFunction[nCapitalNextPeriod,nProductivity];\n",
    "\n",
    "                if valueProvisional>valueHighSoFar:\n",
    "                    valueHighSoFar = valueProvisional\n",
    "                    capitalChoice = vGridCapital[nCapitalNextPeriod]\n",
    "                    gridCapitalNextPeriod = nCapitalNextPeriod\n",
    "                else:\n",
    "                    break # We break when we have achieved the max\n",
    "\n",
    "\n",
    "            mValueFunctionNew[nCapital,nProductivity] = valueHighSoFar\n",
    "            mPolicyFunction[nCapital,nProductivity]   = capitalChoice\n",
    "\n",
    "    maxDifference = (abs(mValueFunctionNew-mValueFunction)).max()\n",
    "\n",
    "    mValueFunction    = mValueFunctionNew\n",
    "    mValueFunctionNew = zeros((nGridCapital,nGridProductivity),dtype=float)\n",
    "\n",
    "    iteration += 1\n",
    "    if(iteration%10 == 0 or iteration == 1):\n",
    "        print \" Iteration = \", iteration, \", Sup Diff = \", maxDifference\n",
    "        \n",
    "print \" Iteration = \", iter, \", Sup Duff = \", maxDifference\n",
    "print \" \"\n",
    "print \" My Check = \", mPolicyFunction[1000-1,3-1]\n",
    "print \" \"\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print \"Elapse time = is \", t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NAF Approach\n",
    "\n",
    "Instead of discritizing the policy function we represent the utility of state $S$ (in this case capital $k_t$ and technology $z_t$) using a neural network $V(k_t, z_t)$.  The planners problem is then to select $c_t, k_{t+1}$ to maximize \n",
    "\n",
    "$$ \\log{c_t} + \\beta  \\mathbb{E} V_{k_{t+1}, z_{t+1}}$$\n",
    "\n",
    "However, if $V(k, z)$ is arbritary there will not be an analytical way to solve for (note is this true??) $c_t$.  Therefore, we limit the functional form of $V$ in the following way\n",
    "\n",
    "$$ Q(z_t, k_t, c_t) = A(z_t, k_t, c_t) +_ V(z_t, k_t)$$\n",
    "$$ A(z_t, k_t, c_t) = (c_t - \\mu(z_t, k_t))' P(k_t) (c_t - \\mu(z_t, k_t))$$\n",
    "\n",
    "$P$ is a state-dependent positive-definite square matrix. This ensures that the maximal value of $A$ is at most 0, so the optimal policy function is to always make select $c_t = \\mu(z_t, k_t)$.\n",
    "\n",
    "We can now apply value iteration on arbritary points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
